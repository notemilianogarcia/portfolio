---
title: "PocketGuide"
summary: "Domain-adapted LLM for structured travel guidance with evaluation-first design, synthetic instruction tuning, and offline inference."
date: "2026-01-28"
tags: ["Deep Learning", "Evaluation", "Python", "Machine Learning", "LLM", "Fine-tuning"]
featured: false
---

# PocketGuide

Domain-adapted language model for structured travel guidance. Built with evaluation-first methodology, synthetic instruction tuning, and quantized for offline inference.

## TL;DR

- **Evaluation-first LLM adaptation** with fixed 20-prompt benchmark suite and objective metrics (parse success, schema compliance, uncertainty markers)
- **Synthetic instruction pipeline** using teacher-student generation with multi-stage quality gating and cost-controlled OpenRouter backend
- **LoRA fine-tuning** on Llama-2-7B with 5 documented iterations improving parse success from 80% to 100%
- **Structured output contracts** (JSON envelope + typed payloads) validated at inference time for reliable schema compliance
- **GGUF quantization** via llama.cpp for offline deployment on consumer hardware

## What I Built

The system addresses a specific ML challenge: adapting a general-purpose 7B model to produce reliable, schema-compliant JSON for travel planning—under constraints that matter in practice.

**Evaluation Infrastructure**

I defined output contracts and benchmarks before training. A fixed 20-prompt suite measures parse success, schema compliance, and uncertainty marker presence. All training iterations are evaluated against these contracts with timestamped runs and artifact archiving for reproducibility.

**Synthetic Data Pipeline**

Built a three-stage teacher-student pipeline for training data generation:

- **Prompt planning** with spec-driven diversity across categories, regions, and difficulty levels
- **Draft generation** using OpenRouter with rate limiting (15 RPM) and multi-model fallback
- **Quality gating** with critique-based filtering and acceptance thresholds

The pipeline produces balanced datasets with full provenance tracking (config snapshots, prompt hashes, token counts).

**Output Contracts**

Implemented JSON schema validation with two layers:

- **Envelope schema (v0)** enforcing consistent structure: summary, assumptions, uncertainty notes, verification steps
- **Payload schemas (v1)** for domain-specific outputs: itinerary, checklist, decision_tree, procedure

Validation occurs at inference time with strict and lenient parsing modes.

**Parameter-Efficient Fine-Tuning**

LoRA adapters on Llama-2-7B with config-driven training, gradient checkpointing, and checkpoint management. Five training iterations (v1–v5) targeted specific failure modes identified through evaluation, with each intervention measured and documented.

**Local Inference Runtime**

GGUF quantization enables offline inference on consumer hardware. A registry-driven model selection system maps logical names to quantized artifacts. The runtime supports both Hugging Face + PEFT for evaluation and llama.cpp for production deployment.

---

## Why It Matters

**ML/Research Discipline**

Evaluation-first development ensures all improvements are measured objectively. Fixed benchmarks, deterministic seeding, and versioned schemas provide reproducible baselines. Structured outputs with uncertainty markers demonstrate domain adaptation beyond generic chat capabilities.

**ML Engineering**

The system demonstrates production ML practices: synthetic data generation at scale, parameter-efficient fine-tuning with experiment tracking, quantization for deployment constraints, and iterative improvement based on measured failure modes. All pipeline stages are config-driven with artifact provenance.

**Software Engineering**

Clean architecture with modular separation: evaluation, data generation, training, inference. Comprehensive test coverage across all pipeline stages. Makefile provides stable commands for consistent developer experience. Type hints and docstrings throughout.

---

## Architecture

The system follows a contracts-first, evaluation-driven design:

<code style={{ display: 'block', whiteSpace: 'pre', margin: '0 0 1rem 0' }} className="bg-surface-2 border border-border rounded-lg p-4 overflow-x-auto text-sm font-mono text-text-2">{`Output Schemas (Envelope + Payloads)
      ↓
[Benchmark Suite: 20 prompts → Fixed Eval]
      ↓
[Prompt Planning] → [Teacher Generation (OpenRouter)]
      ↓                       ↓
[Draft Generation]      [Quality Gating]
      ↓                       ↓
[Dataset v1-v5] ←──────┘
      ↓
[LoRA Training: Llama-2-7B + PEFT]
      ↓
[Checkpoint] → [Evaluation Run]
      ↓                ↓
[Metrics]        [Failure Analysis]
      ↓                ↓
[Iteration v+1] ←┘
      ↓
[GGUF Quantization (llama.cpp)]
      ↓
[Local Inference Runtime]`}</code>

---

## Results

Five training iterations demonstrated measurable improvement on the held-out evaluation suite:

| Metric | v1 | v3 | v5 | Target |
|--------|----|----|----|----|
| **Parse Success** | 0.80 | 1.00 | 1.00 | 1.00 ✓ |
| **Uncertainty Markers** | 0.85 | 1.00 | 1.00 | 1.00 ✓ |
| **Envelope Fields** | 0.00 | 0.15 | 0.20 | 1.00 |

**Key findings:**

- Parse success improved from 80% to 100% through data quality interventions and training duration adjustments
- Uncertainty marker presence reached 100%, ensuring reliable acknowledgment of assumptions
- Envelope field compliance improved to 20% but requires further architectural exploration

The iteration process confirmed that structural compliance responds to targeted data interventions while full schema enforcement requires additional objective design. Quantized models (Q4_K_M) maintain acceptable latency for interactive use.

---

## Reliability & Safety

PocketGuide enforces uncertainty by design through required schema fields (`uncertainty_notes`, `verification_steps`). The system is a research prototype for demonstrating domain adaptation and structured output generation—not an authoritative travel information source.

Training data has a knowledge cutoff; visa requirements, border policies, and local conditions change. Users must verify time-sensitive or safety-critical information with official government sources.

---

## Future Directions

**Schema Compliance Improvements** — Explore loss weighting, constrained decoding, or architectural changes to improve envelope field coverage beyond 20%.

**JSON Repair** — Post-processing to recover from minor format violations without full regeneration.

**Larger Model Exploration** — Adapter training on 13B+ base models to assess capacity vs efficiency trade-offs.

**Server Mode** — FastAPI wrapper for persistent inference service with request logging and performance measurement.

**Multi-Domain Benchmarks** — Expand evaluation beyond travel to test generalization of structured output methodology.

<div className="mt-12 pt-8 border-t border-border">
  <div className="flex flex-wrap gap-4">
    <a 
      href="https://github.com/notemilianogarcia/pocket-guide"
      target="_blank"
      rel="noopener noreferrer"
      className="inline-flex items-center gap-2 rounded-lg bg-accent px-6 py-3 text-sm font-bold text-bg hover:opacity-90 transition-opacity"
    >
      <svg className="w-5 h-5 shrink-0" fill="currentColor" viewBox="0 0 24 24">
        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
      </svg>
      <span>View on GitHub</span>
    </a>
    <a 
      href="/blog/pocketguide-evaluation-first"
      className="inline-flex items-center gap-2 rounded-lg px-6 py-3 text-sm font-bold text-bg hover:opacity-90 transition-opacity"
      style={{ backgroundColor: '#FF5370' }}
    >
      <svg className="w-5 h-5 shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
      </svg>
      <span>Read Blog Post</span>
    </a>
  </div>
</div>
