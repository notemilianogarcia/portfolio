---
title: "RAG Benchmark Service"
summary: "Evaluation-first RAG system with dual retrieval baselines (BM25 + FAISS), production FastAPI service, comprehensive testing (239 tests), drift monitoring, and full ML lifecycle automation."
date: "2026-01-26"
tags: ["RAG", "Evaluation", "ML Engineering", "MLOps", "Information Retrieval", "Python", "FastAPI", "BM25", "FAISS"]
featured: true
---

# RAG Benchmark Service

Benchmark-driven retrieval system for scientific question answering. Built with evaluation-first discipline, dual retrieval baselines, and production MLOps infrastructure.

## TL;DR

- Evaluation-first RAG system using **SciFact benchmark** (5,183 docs, 300 test queries)
- **Dual retrieval baselines** (BM25 + dense/FAISS) with offline metrics and comparison reports
- **Production FastAPI service** with grounded answer generation and citation enforcement
- Full ML lifecycle: **drift detection, scheduled evaluation, gated promotion**
- **239 tests** covering validation, retrieval, API contracts, and end-to-end RAG flows

## What I Built

The system follows a clear data → retrieval → evaluation → serving → monitoring pipeline.

**Data & Benchmarking**

I integrated the **SciFact dataset** from the BEIR benchmark suite with full validation and statistics reporting. The pipeline downloads corpus documents, test queries, and relevance judgments (qrels), then validates schema consistency and referential integrity.

**Retrieval Baselines**

Built two complementary retrieval approaches to understand trade-offs between lexical and semantic search:

- **BM25 baseline** with deterministic indexing and content-addressed artifacts
- **Dense retrieval** using sentence-transformers embeddings with FAISS flat index

Both retrievers expose a unified interface for plug-and-play comparison.

**Evaluation Harness**

Offline metrics drive all decisions. I implemented **Recall@K**, **nDCG@K**, and **MRR@K** with side-by-side retriever comparison and delta reporting. Results are timestamped and archived for reproducibility.

**RAG Generation**

The system integrates with **OpenRouter** for LLM inference, enforcing strict **citation discipline** in prompts. When retrieval quality is weak, the system abstains with "Insufficient evidence" rather than hallucinating.

**Production Service**

Deployed as a **FastAPI service** with four endpoints: `/health`, `/v1/retrieve`, `/v1/query`, and `/metrics`. All requests include timing instrumentation and are logged to JSONL for offline analysis.

**ML Lifecycle & Monitoring**

Continuous improvement is automated through several mechanisms:

- **Drift detection** using PSI to track query length and similarity score distributions
- **Scheduled evaluation** with configurable query sampling every Sunday at 03:00 UTC
- **Gated promotion** requiring 2% relative nDCG@10 improvement with no Recall@10 regression
- **GitHub Actions** orchestrating eval, drift checks, and promotion decisions

Candidate retrievers must pass metric thresholds before updating the production pointer file.

---

## Why It Matters

**ML/Research Discipline**

I prioritized reproducibility and rigorous evaluation over quick wins. The system uses a standard **BEIR dataset** with held-out evaluation sets and qrels. Metrics capture multiple dimensions: **Recall@K** for coverage, **nDCG** for ranking quality, **MRR** for first-hit performance. Grounded prompts enforce citations and abstention when evidence is insufficient.

**ML Engineering & MLOps**

Production ML systems require versioning, monitoring, and gated deployments. Indexes are identified by **SHA256 content hash**. **PSI-based drift detection** tracks distribution shifts. The **production pointer pattern** enables atomic updates with full audit trails. **GitHub Actions** runs weekly evaluation with artifact uploads.

**Software Engineering**

The codebase follows clean architecture principles with modular separation of concerns and a **factory pattern** for retriever interchangeability. **239 tests** cover data validation, retrieval correctness, API contracts, and end-to-end RAG smoke tests. Developer experience is prioritized with **Makefile targets**, **Docker** for local serving, and environment-based configuration.

---

## Architecture

The system follows a linear pipeline with continuous feedback loops for model improvement.

<code style={{ display: 'block', whiteSpace: 'pre', margin: '0 0 1rem 0' }} className="bg-surface-2 border border-border rounded-lg p-4 overflow-x-auto text-sm font-mono text-text-2">{`SciFact Dataset
      ↓
[Data Pipeline: Download → Validate → Stats]
      ↓
[Indexing: BM25 + Dense/FAISS]
      ↓
[Retriever Factory] → [FastAPI Service]
      ↓                       ↓
[Offline Eval]          [/v1/query] → [Request Logs (JSONL)]
      ↓                       ↓
[Scheduled Eval]        [Drift Detection (PSI)]
      ↓                       ↓
[Promotion Gate] ←─────┘
      ↓
[Production Pointer File] → [API Reloads Retriever]`}</code>

---

## Results

I evaluated both retrievers on **SciFact** (300 queries with labeled relevance judgments):

| Retriever | Recall@10 | nDCG@10 | MRR@10 |
|-----------|-----------|---------|--------|
| **BM25**  | 0.776     | 0.652   | 0.619  |
| **Dense** | 0.783     | 0.645   | 0.605  |

**Dense retrieval** achieves **+0.8% higher recall** (better coverage), while **BM25** shows **+1.1% higher nDCG** (better ranking quality). Both are viable baselines; the choice depends on domain requirements and optimization targets.

All results are reproducible via `make eval_bm25` and `make eval_dense`.

---

## Reliability & Safety

The system enforces strict **citation discipline** through prompt engineering. All claims must reference retrieved documents, and the system abstains with "Insufficient evidence" when retrieval quality is low. End-to-end tests verify citation correctness.

This is a demonstration system. Users must verify critical information independently.

---

## Future Directions

The roadmap focuses on evaluation rigor and production readiness:

**Answer Quality Evaluation** — Automated groundedness testing, citation coverage metrics, and regression detection for generation quality.

**Reranker Integration** — Cross-encoder second-stage ranking to improve relevance beyond first-stage retrieval.

**Multi-Domain Benchmarks** — Expand evaluation to additional BEIR datasets (NFCorpus, FiQA) to test generalization.

**CI/CD Enhancements** — PR-level smoke evaluation with quality gates, Slack/email alerting on drift or regression.

**Production Hardening** — API key management, rate limiting, and authentication layer for deployment.

<div className="mt-12 pt-8 border-t border-border">
  <div className="flex flex-wrap gap-4">
    <a 
      href="https://github.com/notemilianogarcia/rag-benchmark-service"
      target="_blank"
      rel="noopener noreferrer"
      className="inline-flex items-center gap-2 rounded-lg bg-accent px-6 py-3 text-sm font-bold text-bg hover:opacity-90 transition-opacity"
    >
      <svg className="w-5 h-5 shrink-0" fill="currentColor" viewBox="0 0 24 24">
        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
      </svg>
      <span>View on GitHub</span>
    </a>
    <a 
      href="/blog/rag-benchmark-service"
      className="inline-flex items-center gap-2 rounded-lg px-6 py-3 text-sm font-bold text-bg hover:opacity-90 transition-opacity"
      style={{ backgroundColor: '#FF5370' }}
    >
      <svg className="w-5 h-5 shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
      </svg>
      <span>Read Blog Post</span>
    </a>
  </div>
</div>

